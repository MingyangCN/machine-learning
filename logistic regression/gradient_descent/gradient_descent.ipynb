{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 梯度下降\n",
    "[知乎](https://zhuanlan.zhihu.com/p/77380412)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Gradient Descent（BGD，批量梯度下降）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGradientDescent:\n",
    "    def __init__(self, eta=0.01, n_iter=1000, tolerance=0.001):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        X = np.c_[np.ones(n_samples), X]  # 增加截距项\n",
    "        n_features = X.shape[-1]\n",
    "\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            errors = X.dot(self.theta) - y\n",
    "            loss = 1 / (2 * n_samples) * errors.dot(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "            else:\n",
    "                gradient = 1 / n_samples * X.T.dot(errors)\n",
    "                self.theta -= self.eta * gradient\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Gradient Descent（SGD，随机梯度下降）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StochasticGradientDescent(BatchGradientDescent):\n",
    "    def __init__(self, shuffle=True, random_state=None, **kwargs):\n",
    "        super(StochasticGradientDescent, self).__init__(**kwargs)\n",
    "        self.shuffle = shuffle\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)  # 重新排序\n",
    "            errors = []\n",
    "            for xi, yi in zip(X, y):\n",
    "                error_i = xi.dot(self.theta) - yi\n",
    "                errors.append(error_i**2)\n",
    "                gradient_i = xi.T.dot(error_i)  # 单个样本的梯度\n",
    "                self.theta -= self.eta * gradient_i\n",
    "            loss = 1/2 * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _shuffle(X, y):\n",
    "        location = np.random.permutation(len(y))\n",
    "        return X[location], y[location]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mini-Batch Gradient Descent（MBGD，小批量梯度下降）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MiniBatchGradientDescent(StochasticGradientDescent):\n",
    "    def __init__(self, batch_size=10, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        super(MiniBatchGradientDescent, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y  # 长度与batch_size的长度一致\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)  # 小批量样本梯度\n",
    "                self.theta -= self.eta * mini_gradient\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Moment Gradient Descent（MGD，动量梯度下降）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " class MomentumGradientDescent(MiniBatchGradientDescent):\n",
    "    def __init__(self, gamma=0.9, **kwargs):\n",
    "        self.gamma = gamma                # 当gamma=0时，相当于小批量随机梯度下降\n",
    "        super(MomentumGradientDescent, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.velocity = np.zeros_like(self.theta)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                self.velocity = self.velocity * self.gamma + self.eta * mini_gradient\n",
    "                self.theta -= self.velocity\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adaptive Gradient Descent（AdaGrad，自适应梯度下降，2011）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaptiveGradientDescent(MiniBatchGradientDescent):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        self.epsilon = epsilon\n",
    "        super(AdaptiveGradientDescent, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        gradient_sum = np.zeros(n_features)\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                gradient_sum += mini_gradient ** 2\n",
    "                adj_gradient = mini_gradient / (np.sqrt(gradient_sum + self.epsilon))\n",
    "                self.theta -= self.eta * adj_gradient\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adaptive Delta Gradient Descent（AdaDelta，自适应调整梯度下降, 2012）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaDelta(MiniBatchGradientDescent):\n",
    "    def __init__(self, gamma=0.95, epsilon=1e-6, **kwargs):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        super(AdaDelta, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        gradient_exp = np.zeros(n_features)\n",
    "        delta_theta_exp = np.zeros(n_features)\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                gradient_exp = self.gamma * gradient_exp + (1 - self.gamma) * mini_gradient ** 2\n",
    "                gradient_rms = np.sqrt(gradient_exp + self.epsilon)\n",
    "                delta_theta = -np.sqrt(delta_theta_exp + self.epsilon) / gradient_rms * mini_gradient\n",
    "                delta_theta_exp = self.gamma * delta_theta_exp + (1 - self.gamma) * delta_theta ** 2\n",
    "                delta_theta_rms = np.sqrt(delta_theta_exp + self.epsilon)\n",
    "                delta_theta = -delta_theta_rms / gradient_rms * mini_gradient\n",
    "                self.theta += delta_theta\n",
    "\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Root Mean Square Prop（RMSProp，均方根支撑， 2012）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RMSProp(MiniBatchGradientDescent):\n",
    "    def __init__(self, gamma=0.9, epsilon=1e-6, **kwargs):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        super(RMSProp, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        gradient_exp = np.zeros(n_features)\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                gradient_exp = self.gamma * gradient_exp + (1 - self.gamma) * mini_gradient ** 2\n",
    "                gradient_rms = np.sqrt(gradient_exp + self.epsilon)\n",
    "                self.theta -= self.eta / gradient_rms * mini_gradient\n",
    "\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nesterov Accelerated Gradient Descent（NAG，Nesterov加速梯度下降，2013）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NesterovAccelerateGradient(MomentumGradientDescent):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NesterovAccelerateGradient, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.velocity = np.zeros_like(self.theta)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta - self.gamma * self.velocity) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                self.velocity = self.velocity * self.gamma + self.eta * mini_gradient\n",
    "                self.theta -= self.velocity\n",
    "                loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adaptive Moment Estimation（Adam，自适应矩估计，2014）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaptiveMomentEstimation(MiniBatchGradientDescent):\n",
    "    def __init__(self, beta_1=0.9, beta_2=0.999, epsilon=1e-6, **kwargs):\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        super(AdaptiveMomentEstimation, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        m_t = np.zeros(n_features)\n",
    "        v_t = np.zeros(n_features)\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                m_t = self.beta_1 * m_t + (1 - self.beta_1) * mini_gradient\n",
    "                v_t = self.beta_2 * v_t + (1 - self.beta_2) * mini_gradient ** 2\n",
    "                m_t_hat = m_t / (1 - self.beta_1 ** self.i)  # correction\n",
    "                v_t_hat = v_t / (1 - self.beta_2 ** self.i)\n",
    "                self.theta -= self.eta / (np.sqrt(v_t_hat) + self.epsilon) * m_t_hat\n",
    "\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adaptive Moment Estimation Max（AdaMax, 2015）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaMax(AdaptiveMomentEstimation):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AdaMax, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        m_t = np.zeros(n_features)\n",
    "        u_t = np.zeros(n_features)\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                m_t = self.beta_1 * m_t + (1 - self.beta_1) * mini_gradient\n",
    "                m_t_hat = m_t / (1 - self.beta_1 ** self.i)\n",
    "                u_t = np.max(np.c_[self.beta_2 * u_t, np.abs(mini_gradient)], axis=1)\n",
    "                self.theta -= self.eta / u_t * m_t_hat\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nesterov Adaptive Moment Estimation（Nadam，Nesterov加速自适应矩估计，2016）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Nadam(AdaptiveMomentEstimation):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Nadam, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        m_t = np.zeros(n_features)\n",
    "        v_t = np.zeros(n_features)\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                m_t = self.beta_1 * m_t + (1 - self.beta_1) * mini_gradient\n",
    "                v_t = self.beta_2 * v_t + (1 - self.beta_2) * mini_gradient ** 2\n",
    "                m_t_hat = m_t / (1 - self.beta_1 ** self.i)  # correction\n",
    "                v_t_hat = v_t / (1 - self.beta_2 ** self.i)\n",
    "                self.theta -= self.eta / (np.sqrt(v_t_hat) + self.epsilon) * (\n",
    "                            self.beta_1 * m_t_hat + (1 - self.beta_1) * mini_gradient / (1 - self.beta_1 ** self.i))\n",
    "\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adam & RMSProp Gradient Descent (AMSGrad, 2018)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AMSGrad(AdaptiveMomentEstimation):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AMSGrad, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        m_t = np.zeros(n_features)\n",
    "        v_t = np.zeros(n_features)\n",
    "        v_t_hat = np.zeros(n_features)\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)\n",
    "                m_t = self.beta_1 * m_t + (1 - self.beta_1) * mini_gradient\n",
    "                v_t = self.beta_2 * v_t + (1 - self.beta_2) * mini_gradient ** 2\n",
    "                v_t_hat = np.max(np.hstack((v_t_hat, v_t)))\n",
    "                self.theta -= self.eta / (np.sqrt(v_t_hat) + self.epsilon) * m_t\n",
    "\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}